%
% File report.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{report}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{float}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{placeins}
\restylefloat{table}

\lstset{
  language=C,                % choose the language of the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  basicstyle=\ttfamily\scriptsize,
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=true,         % sets if automatic breaks should only happen at whitespace
}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{CS267 Assignment 2: Parallelize Particle Simulation}

\author{Hang Su \\
  Dept. of EECS, UC Berkeley \\
  International Computer Science Institute \\
  {\tt suhang3240@gmail.com}
  \And
  Andreas Borgen Longva \\
  Dept. of Mathematics, UC Berkeley \\
  NTNU \\
  {\tt andreas.b.longva@gmail.com}
}
\date{}

\begin{document}
\maketitle
\vspace{-8mm}
\begin{abstract}
This report investigates the parallelization of particle simulation using different parallelization models.
OpenMP, Pthreads, MPI and CUDA are used for parallelization, and their scaling performances are compared.
We show that these techniques can effectively speed up the simulation, achieving an 40\% average scaling efficiency.
percentage of Peak could be achieved.
\end{abstract}

\section{Introduction}
Particle simulation is commonly used in many branches of science, including mechanics, biology, astronomy, etc. In this assignment, a toy particle simulator is parallelized using different parallelization models.

Naive particle simulation has a computational complexity of $O(n^2)$ in time, where $n$ is the number of particles in the simulation. This follows from the fact that every particle interacts with every other particle. For some problems, the interaction between distant particles is negligible or even completely non-existent. This is the case for the problem we are given in this assignment.

Exploiting this property, the computational complexity may be brought down to $O(n)$. One way to accomplish this is spatial partitioning, or \emph{binning}. By partitioning the space into a set of appropriately sized bins, a particle only needs to interact with particles in the same or nearby bins.

There are many ways to implement this, but most - if not all - of them belong to one of two categories: \emph{stateless} and \emph{stateful} solutions. In the stateless approach, an independent problem is solved on every time iteration. That is, the partitioning process is run completely for every time step, and each particle is assigned to a bin. In contrast, the stateful approach initially distributes particles into bins, but for every timestep, only the movement of particles are communicated between partitions. As such, each partition remembers which particles belong to that partition, which constitutes the state of the simulation.

\section{Serial implementation}
Pseudo-code for the naive $O(n^2)$ approach is presented in table \ref{tab:naive}. The straightforward way to improve on this by means of spatial partitioning is conceptually quite simple, and illustrated in table \ref{tab:serial}.

\begin{table}[htb]
  \centering
  \begin{tabular}{l}
    \hline
      Naive particle simulation\\
    \hline
      \lstinputlisting{samples/naive.cpp}\\
    \hline
  \end{tabular}
  \label{tab:naive}
\end{table}

\begin{table}[htb]
  \centering
  \begin{tabular}{l}
    \hline
      Binned particle simulation\\
    \hline
      \lstinputlisting{samples/serial.cpp}\\
    \hline
  \end{tabular}
  \label{tab:serial}
\end{table}

\FloatBarrier

\section{Parallelization}
\subsection{Shared Memory Models}
OpenMP and Pthreads are two progamming models for shared memory parallelization. We implement both of them and try to 
compare their performance. Synchronization is done after each of the parallel block to eliminate race condition. Lock
is used for parallel updating bin information.

\begin{table}[htb]
  \centering
  \begin{tabular}{l}
    \hline
      Shared Memory Parallelization\\
    \hline
      \lstinputlisting{samples/shared_memory.cpp}\\
    \hline
  \end{tabular}
  \label{tab:shared_memory}
\end{table}

\subsection{Distributed Memory Model}
We use MPI \cite{mpi} as distributed memory parallelization model in this section. The strategy is that we first broadcast
all the particles information to all the nodes, and then we bin particles in each of these nodes. And finally, we 
compute forces for a subset of particles in each node and move them. The pesudo code is shown as below.

\begin{table}[htb]
  \centering
  \begin{tabular}{l}
    \hline
      MPI Parallelization\\
    \hline
      \lstinputlisting{samples/mpi_naive.cpp}\\
    \hline
  \end{tabular}
  \label{tab:mpi_naive}
\end{table}

\subsection{CUDA}
The CUDA implementation is like the openMP approach, as it could be seen as a shared memory parallelization model.
The main difference here is that instead of allocating memories dynamically during particle binning, we allocate
memory for bins before simulation begins (this requires an estimate of maximum number of particles in each bin, and
we approximate this by recording that in simulation of serial code). During the simulation, we use atomicAdd to 
track the status of each bin, and assign particle address to bins using the return value of atomicAdd function.

\section{Experiments}
\subsection{Complexity}
Figure~\ref{fig:loglogOn} shows the log-log scale plot for serial and parallel codes run in O(n) time.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{graphics/loglogOn.png}
  \caption{Log-log scale plot for serial and parallel codes}
  \label{fig:loglogOn}
\end{figure}

\subsection{Speedup}
Figure~\ref{fig:speedup} shows the speed up factor for different parallel implementation when number of particles is 500.
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{graphics/speedup.png}
  \caption{Speedup plot for parallel codes}
  \label{fig:speedup}
\end{figure}

As is shown in the figure, pthreads perform worst in this setup. But if we increase the number of particles to 16000,
we end up with similar speedups for pthreads and openMP (Figure~\ref{fig:speedup-16000}).
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{graphics/speedup-16000.png}
  \caption{Speedup plot for parallel codes (num particles = 16000)}
  \label{fig:speedup-16000}
\end{figure}

\subsection{Weak scaling}

\subsection{CUDA}
Figure~\ref{fig:gpu} shows the performance of our code versus the naive GPU code.
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{graphics/gpu.png}
  \caption{Log-log plot for gpu performance)}
  \label{fig:gpu}
\end{figure}

\section{Discussions}
\subsection{Pthreads v.s. openMP v.s. CUDA}
OpenMP is easier to implement and the overhead is well-managed. However, Pthreads is more flexible -- one could call
different routines together. In general, these two methods are similar to each other in that they are similar memory
sharing strategy. CUDA is more powerful for massive parallelization. However, it requires memory management for devices,
and it is a bit difficult to scale up.

\subsection{Shared memory model v.s. distributed memory model}
Shared memory models are easier to implement, and they tend to give better speedup performance. However, distributed memory 
models can scale up massively.

\section{Conclusion}
Different parallelization models are shown to be effective in parallelizing particle simulations. Of the four parallelization models,
CUDA is shown to be most effective in the simulation. Pthreads and openMP may achieve an average of 40\% scaling factor, while
MPI is 

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{report}

\end{document}
