%
% File report.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{report}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{float}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{placeins}
\restylefloat{table}

\lstset{
  language=C,                % choose the language of the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  basicstyle=\ttfamily\scriptsize,
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=true,         % sets if automatic breaks should only happen at whitespace
}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{CS267 Assignment 2: Parallelize Particle Simulation}

\author{Hang Su \\
  Dept. of EECS, UC Berkeley \\
  International Computer Science Institute \\
  {\tt suhang3240@gmail.com}
  \And
  Andreas Borgen Longva \\
  Dept. of Mathematics, UC Berkeley \\
  NTNU \\
  {\tt andreas.b.longva@gmail.com}
}
\date{}

\begin{document}
\maketitle
\vspace{-8mm}
\begin{abstract}
This report investigates the parallelization of particle simulation using different parallelization models.
OpenMP, Pthreads, MPI and CUDA are used for parallelization, and their scaling performances are compared.
We show that these techniques can effectively speed up the simulation, achieving an 40\% average scaling efficiency.
percentage of Peak could be achieved.
\end{abstract}

\section{Introduction}
Particle simulation is commonly used in many branches of science, including mechanics, biology, astronomy, etc. In this assignment, a toy particle simulator is parallelized using different parallelization models.

Naive particle simulation has a computational complexity of $O(n^2)$ in time, where $n$ is the number of particles in the simulation. This follows from the fact that every particle interacts with every other particle. For some problems, the interaction between distant particles is negligible or even completely non-existent. This is the case for the problem we are given in this assignment.

Exploiting this property, the computational complexity may be brought down to $O(n)$. One way to accomplish this is spatial partitioning, or \emph{binning}. By partitioning the space into a set of appropriately sized bins, a particle only needs to interact with particles in the same or nearby bins.

There are many ways to implement this, but most - if not all - of them belong to one of two categories: \emph{stateless} and \emph{stateful} solutions. In the stateless approach, an independent problem is solved on every time iteration, in the sense that the partitioning scheme merely operates on the full set of particles. That is, the partitioning process is run completely for every time step, and each particle is assigned to a bin. In contrast, the stateful approach initially distributes particles into bins, but for every timestep, only the movement of particles are communicated between partitions. As such, each partition remembers which particles belong to that partition, which constitutes the state of the simulation.

As it is both conceptually simpler and easier to implement, we focus on implementing stateless solutions. Towards the end of the report, we will discuss an algorithm that employs a stateful approach to potentially improve upon our work.

\section{Serial implementation}
Pseudo-code for the naive $O(n^2)$ approach is presented in table \ref{tab:naive}. The straightforward way to improve on this by means of spatial partitioning is conceptually quite simple, and illustrated in table \ref{tab:serial}.

\begin{table}[htb]
  \centering
  \begin{tabular}{l}
    \hline
      Naive particle simulation\\
    \hline
      \lstinputlisting{samples/naive.cpp}\\
    \hline
  \end{tabular}
  \label{tab:naive}
\end{table}

\begin{table}[htb]
  \centering
  \begin{tabular}{l}
    \hline
      Binned particle simulation\\
    \hline
      \lstinputlisting{samples/serial.cpp}\\
    \hline
  \end{tabular}
  \label{tab:serial}
\end{table}

\FloatBarrier

\section{Parallelization}
\subsection{Shared Memory Models}
OpenMP and Pthreads are two progamming models for shared memory parallelization. We implement both of them and try to compare their performance. See table \ref{tab:shared_memory} for pseudo code of our implementation. Synchronization is done after each one of the parallel blocks to eliminate race conditions. In order to allow for assigning the particles to bins in parallel, each bin has an associated lock. With a bin-size roughly equal to the cutoff size for particle interaction, we have that the number of bins, $b$, is usually much higher than the number of particles, $n$, i.e. $b >> n$. Thus most bins will be empty, and the contention for any given bin is likely to be very low. Still, there is likely a non-negligible overhead associated with locking and unlocking uncontended locks that frequently. See a discussion on an alternative parallel algorithm towards the end of this paper.

\begin{table}[htb]
  \centering
  \begin{tabular}{l}
    \hline
      Shared Memory Parallelization\\
    \hline
      \lstinputlisting{samples/shared_memory.cpp}\\
    \hline
  \end{tabular}
  \label{tab:shared_memory}
\end{table}

\FloatBarrier

\subsection{Distributed Memory Model}
The distributed memory model that is explored in this assignment revolves around parallelizing our particle simulation using MPI. Our current MPI implementation is fairly naive. For every timestep, every node gets access to all the particles in the system, and then is assigned a share proportional to how many nodes there are. The node then bins all particles, before it computes forces for its local share of particles. After moving, the data is synchronized again in the next step. The pseudo code for the procedure can be found in table \ref{tab:mpi_naive}. Collective communication is used across the board, specifically MPI\_Scatterv and MPI\_Allgatherv.

It is intuitively obvious that there is a lot of redundant communication going on using this scheme. In addition, every node spends a fair amount of processing power on doing the exact same task, namely partitioning the entire domain. Drawing from the experience of building this naive approach, we have come up with some ideas for how we might be able to alleviate this problem, which will be covered in a later section.

\begin{table}[htb]
  \centering
  \begin{tabular}{l}
    \hline
      MPI Parallelization\\
    \hline
      \lstinputlisting{samples/mpi_naive.cpp}\\
    \hline
  \end{tabular}
  \label{tab:mpi_naive}
\end{table}

\subsection{CUDA}
The CUDA implementation is like the openMP approach, as it could be seen as a shared memory parallelization model.
The main difference here is that instead of allocating memories dynamically during particle binning, we allocate
memory for bins before simulation begins (this requires an estimate of maximum number of particles in each bin, and
we approximate this by recording that in simulation of serial code). During the simulation, we use atomicAdd to 
track the status of each bin, and assign particle address to bins using the return value of atomicAdd function.

\section{Experiments}
\subsection{Complexity}
Figure~\ref{fig:loglogOn} shows the log-log scale plot for serial and parallel codes run in O(n) time for a relatively small number of particles. Figure~\ref{fig:loglogOn_largescale} illustrates the same, but for larger scale problems.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{graphics/loglogOn.png}
  \caption{Log-log scale plot for serial and parallel codes}
  \label{fig:loglogOn}
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{graphics/loglogOn_largescale.png}
  \caption{Log-log scale plot for serial and parallel codes for larger scale simulations. Supplied exponents in the $O(n^x)$ estimate are based on simple linear regression.}
  \label{fig:loglogOn_largescale}
\end{figure}

\subsection{Speedup}
Figure~\ref{fig:speedup} shows the speed up factor for different parallel implementation when number of particles is 500.
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{graphics/speedup.png}
  \caption{Speedup plot for parallel codes}
  \label{fig:speedup}
\end{figure}

As is shown in the figure, Pthreads performs the worst in this setup. But if we increase the number of particles to 16000, the difference between Pthreads and OpenMP are much less pronounced (Figure~\ref{fig:speedup-16000}). 
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{graphics/speedup-16000.png}
  \caption{Speedup plot for parallel codes (num particles = 16000)}
  \label{fig:speedup-16000}
\end{figure}

\subsection{CUDA}
Figure~\ref{fig:gpu} shows the performance of our code versus the naive GPU code.
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{graphics/gpu.png}
  \caption{Log-log plot for gpu performance)}
  \label{fig:gpu}
\end{figure}

\section{Discussions}
\subsection{Pthreads v.s. openMP v.s. CUDA}
OpenMP is easier to implement and the overhead is well-managed. However, Pthreads is more flexible -- one could call different routines together. In general, these two methods are similar to each other in that they are similar memory
sharing strategy. CUDA is more powerful for massive parallelization. However, it requires memory management for devices, and it is a bit difficult to scale up.

\subsection{Shared memory model v.s. distributed memory model}
Shared memory models like OpenMP and Pthreads are easier to implement, and they tend to give better speedup performance. However, it does not scale very well, as it is limited by hardware constraints. Distributed memory solutions like MPI can scale to massively large problems. It is sometimes possible to get the best of both worlds, implementing shared memory parallelization on individual MPI nodes. This is to an extent somethign we experimented with, but the bottlenecks in our MPI implementation seemed to limit the usefulness of this approach.

\subsection{Breakdown of serial computation and implications for MPI}
When we set out to work on the MPI part of the project, we initially assumed that the partitioning of the particles would be significantly cheaper than the computation of forces. This steered our initial design process. After profiling our serial implementation with \emph{Valgrind}, however, and after making necessary optimizations, it turned out that roughly 20 \% of the computation time on every iteration was spent in the partitioning process. Hence, for an MPI implementation, parallelizing the partitioning process is essential. On top of that, the profiling tool $IPM$ confirms that our MPI implementation spends an increasing amount of time communicating as the problem size increases, and so the communication cost of doing the partition for the entire domain on a single node and communicating the result is prohibitively expensive. Unfortunately we never got to the point where we could finish an implementation which deals with this issue, but the next section discusses an algorithm that would hopefully improve upon these weaknesses in our current implementation.

\FloatBarrier

\section{Reducing communication and synchronization}
This section introduces an algorithm that we intended as the next step for our MPI implementation, but never had the time to implement it. Using the terminology from introduction, it is a stateful algorithm in that it subdivides the domain into subdomains which are responsible for the particles that reside in that particular subdomain. No single node has access to all particles at any time during the simulation. The root node scatters the initial data and gathers the result at the end. To hopefully clarify matters, we introduce some terminology that will be used in the algorithm.

\subsection{Terminology}
The \emph{domain} is the physical domain all the particles in the simulation reside in. The domain has a physical size, which in this case is two-dimensional and square.

A \emph{subdomain} is a subset of the domain with the following properties:
\begin{itemize}
\item Contains a set of particles which physically reside in the subdomain.
\item Contains a set of ghost particles, called the ghost layer, which reside in neighboring subdomains and are close enough to the subdomain to interact with the particles inside it.
\item Has a predetermined physical size.
\end{itemize}

\subsection{Description}
Given $n$ particles and $P$ processors, the pseudocode in table \ref{tab:reduce_communication} describes the task to be executed on a single node. Unless otherwise specified, every line is executed by every processor.

\begin{table}[htb]
  \centering
  \begin{tabular}{l}
    \hline
      Reducing Communication\\
    \hline
      \lstinputlisting{samples/reduce_communication.cpp}\\
    \hline
  \end{tabular}
  \label{tab:reduce_communication}
\end{table}

\subsection{MPI-specific details}
The synchronization block in the pseudo code can be implemented as an MPI\_Alltoallv call. The desired effect is that all processors efficiently exchange particles in such a way that each processor is only given the minimum amount of particles it needs to compute the forces for all particles in his domain. All calls outside the synchronization blocks can be calls to functions from the serial implementation, partitioning the subdomain as appropriate, or they can be parallelized using for example OpenMP or CUDA.	

Hopper has 4 ccNUMA nodes on each compute node. The aforementioned algorithm could be a good candidate for running 4 MPI tasks per node, with 6 OpenMP threads running on each MPI node. 

\FloatBarrier
\section{Conclusion}
Different parallelization models are shown to be effective in parallelizing particle simulations. Looking at our implementations of the four parallelization models, CUDA is shown to be most effective in the simulation. Pthreads and OpenMP may achieve an average of 40\% scaling factor for a problem of size $n = 500$. Our implementations leave a lot of room for improvement, particularly the MPI implementation, and we have brought forward some ideas to improve upon our work.1

% include your own bib file like this:
\bibliographystyle{acl}

\end{document}
